{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+8FYNNl7z36uY6Axs+1Qr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maneet2004/Exoplanet-Classification-using-Spiking-Neural-Networks/blob/main/Exoplanet_classification_using_SNNtorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cc1ZfbQ7VEAa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Reshape, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.models import load_model\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exoTrain = pd.read_csv('/content/sample_data/exoTrain.csv')\n",
        "exoTest = pd.read_csv('/content/sample_data/exoTest.csv')\n",
        "\n",
        "exoTrain.tail(5)"
      ],
      "metadata": {
        "id": "P1KB4WIxWHJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(exoTrain['LABEL'].value_counts())\n",
        "print(exoTest['LABEL'].value_counts())"
      ],
      "metadata": {
        "id": "n3VsdJMmtET0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot a specific FLUX column\n",
        "plt.plot(df['FLUX.1'])\n",
        "plt.title('FLUX.1 Over Time')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('FLUX.1')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wNdaWMgvtY57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(exoTrain['LABEL'].value_counts())\n",
        "print(exoTest['LABEL'].value_counts())"
      ],
      "metadata": {
        "id": "BmHbdQ6CtfaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flux_graph(dataset, row, dataframe, planet):\n",
        "    if dataframe:\n",
        "        fig = plt.figure(figsize=(20,5))\n",
        "        ax = fig.add_subplot()\n",
        "        ax.set_title(planet, color='black', fontsize=22)\n",
        "        ax.set_xlabel('time', color='black', fontsize=18)\n",
        "        ax.set_ylabel('flux_' + str(row), color='black', fontsize=18)\n",
        "        ax.grid(False)\n",
        "        flux_time = list(dataset.columns)\n",
        "        flux_values = dataset[flux_time].iloc[row]\n",
        "        ax.plot([i + 1 for i in range(dataset.shape[1])], flux_values, 'black')\n",
        "        ax.tick_params(colors = 'black', labelcolor='black', labelsize=14)\n",
        "        plt.show()\n",
        "    else:\n",
        "        fig = plt.figure(figsize=(20,5))\n",
        "        ax = fig.add_subplot()\n",
        "\n",
        "        ax.set_title(planet, color='black', fontsize=22)\n",
        "        ax.set_xlabel('time', color='black', fontsize=18)\n",
        "        ax.set_ylabel('flux_' + str(row), color='white', fontsize=18)\n",
        "        ax.grid(False)\n",
        "        flux_values = dataset[row]\n",
        "        ax.plot([i + 1 for i in range(dataset.shape[1])], flux_values, 'black')\n",
        "        ax.tick_params(colors = 'black', labelcolor='black', labelsize=14)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "HcBSHpJOtjKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_graph(dataframe, dataset):\n",
        "    with_planet = exoTrain[exoTrain['LABEL'] == 2].head(3).index\n",
        "    wo_planet = exoTrain[exoTrain['LABEL'] == 1].head(3).index\n",
        "\n",
        "    def plot_flux_graph(indices, label):\n",
        "        for row in indices:\n",
        "            flux_graph(dataset, row, dataframe, planet=label)\n",
        "\n",
        "    plot_flux_graph(with_planet, 'Plot for transiting planet')\n",
        "    plot_flux_graph(wo_planet, 'Plot for no transiting planet')\n"
      ],
      "metadata": {
        "id": "5xfSDvestnpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = exoTrain.drop(columns='LABEL')  # Preprocess dataset outside the function call\n",
        "show_graph(dataframe=True, dataset=dataset)\n"
      ],
      "metadata": {
        "id": "4rGKDjfqtvak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_dataset = scaler.fit_transform(exoTrain.drop(columns='LABEL'))\n",
        "\n",
        "show_graph(is_dataframe=False, dataset=scaled_dataset)\n"
      ],
      "metadata": {
        "id": "qlnKbhl1ulpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def handle_outliers(dataset, num_iterations):\n",
        "    \"\"\"\n",
        "    Replaces outlier values in the dataset with the mean value of the respective row.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: pd.DataFrame, the dataset to handle.\n",
        "    - num_iterations: int, number of iterations to repeat the outlier handling process.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with outliers replaced by row means.\n",
        "    \"\"\"\n",
        "    dataset_handled = dataset.copy()  # Avoid modifying the original dataset\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        for index, row in dataset_handled.iterrows():\n",
        "            row_values = row.values\n",
        "            row_mean = row_values.mean()\n",
        "\n",
        "            threshold = 1.5 * np.std(row_values)\n",
        "            outlier_indices = np.where((row_values > row_mean + threshold) | (row_values < row_mean - threshold))\n",
        "\n",
        "            # Replace outliers with the mean value\n",
        "            dataset_handled.iloc[index, outlier_indices] = row_mean\n",
        "\n",
        "    return dataset_handled\n"
      ],
      "metadata": {
        "id": "4Qy92pxbuye4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_dataset = handle_outliers(exoTrain.drop(columns='LABEL'), threshold=2)\n",
        "show_graph(is_dataframe=True, dataset=cleaned_dataset)\n"
      ],
      "metadata": {
        "id": "oI9r9tbWvFys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_labels(y_train, y_test):\n",
        "    \"\"\"\n",
        "    Convert labels: 2 -> 1, 1 -> 0.\n",
        "\n",
        "    Parameters:\n",
        "    train_labels (pd.Series): Labels for the training set.\n",
        "    test_labels (pd.Series): Labels for the test set.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Converted training and test labels.\n",
        "    \"\"\"\n",
        "    binary_label = lambda x: 1 if x == 2 else 0\n",
        "    train_labels_01 = train_labels.apply(binary_label)\n",
        "    test_labels_01 = test_labels.apply(binary_label)\n",
        "\n",
        "    return train_labels_01, test_labels_01\n"
      ],
      "metadata": {
        "id": "AI_9w9CivYiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "def apply_smote(x_train, y_train, over_strategy=0.2, under_strategy=0.3, random_state=17):\n",
        "    \"\"\"\n",
        "    Apply SMOTE and undersampling to balance the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    x_train (pd.DataFrame): Features for the training set.\n",
        "    y_train (pd.Series): Labels for the training set.\n",
        "    over_strategy (float or dict, optional): SMOTE sampling strategy.\n",
        "    under_strategy (float or dict, optional): Random undersampling strategy.\n",
        "    random_state (int, optional): Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Resampled training features and labels.\n",
        "    \"\"\"\n",
        "    smote = SMOTE(sampling_strategy=over_strategy, random_state=random_state)\n",
        "    undersample = RandomUnderSampler(sampling_strategy=under_strategy, random_state=random_state)\n",
        "    pipeline = Pipeline(steps=[('smote', smote), ('undersample', undersample)])\n",
        "\n",
        "    x_train_res, y_train_res = pipeline.fit_resample(x_train, y_train)\n",
        "    return x_train_res, y_train_res\n",
        "\n",
        "# Usage example\n",
        "y_labels = exoTrain['LABEL']\n",
        "print(\"Before SMOTE:\", y_labels.value_counts())\n",
        "\n",
        "x_resampled, y_resampled = apply_smote(handled_dataset, y_labels)\n",
        "print(\"After SMOTE:\", y_resampled.value_counts())\n"
      ],
      "metadata": {
        "id": "Rx5Hfpr5vq6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define training and testing datasets\n",
        "def prepare_datasets(exoTrain, exoTest, outlier_threshold=2, smote_strategy=0.2, under_strategy=0.3, random_state=17):\n",
        "    \"\"\"\n",
        "    Prepare and preprocess the training and testing datasets.\n",
        "\n",
        "    Parameters:\n",
        "    exoTrain (pd.DataFrame): The training dataset.\n",
        "    exoTest (pd.DataFrame): The testing dataset.\n",
        "    outlier_threshold (float, optional): Threshold for handling outliers.\n",
        "    smote_strategy (float or dict, optional): SMOTE oversampling strategy.\n",
        "    under_strategy (float or dict, optional): Random undersampling strategy.\n",
        "    random_state (int, optional): Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Preprocessed training and testing features and labels, number of features.\n",
        "    \"\"\"\n",
        "\n",
        "    x_train, y_train = exoTrain.drop(columns='LABEL'), exoTrain['LABEL']\n",
        "    x_test, y_test = exoTest.drop(columns='LABEL'), exoTest['LABEL']\n",
        "\n",
        "    x_train = handle_outliers(x_train, outlier_threshold)\n",
        "\n",
        "\n",
        "    x_train, y_train = apply_smote(x_train, y_train, over_strategy=smote_strategy, under_strategy=under_strategy, random_state=random_state)\n",
        "\n",
        "    y_train, y_test = convert_labels(y_train, y_test)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    n_features = x_train.shape[1]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test, n_features\n",
        "\n",
        "# Function to graph accuracy and loss\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot the training and validation loss and accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    history: Keras History object containing training history.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ndWgz2W4v4Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(y_test, y_pred, labels=[0, 1], cmap=\"BuGn\", font_scale=1.4, figsize=(10, 7), annot_size=16):\n",
        "    \"\"\"\n",
        "    Plot a confusion matrix using Seaborn's heatmap for better visualization.\n",
        "\n",
        "    Parameters:\n",
        "    y_test (pd.Series or np.array): True labels.\n",
        "    y_pred (pd.Series or np.array): Predicted labels.\n",
        "    labels (list, optional): The labels to display in the confusion matrix.\n",
        "    cmap (str, optional): Colormap for the heatmap.\n",
        "    font_scale (float, optional): Font scale for the annotations.\n",
        "    figsize (tuple, optional): Size of the figure.\n",
        "    annot_size (int, optional): Size of the annotations in the heatmap.\n",
        "\n",
        "    Returns:\n",
        "    np.array: The confusion matrix as a numpy array.\n",
        "    \"\"\"\n",
        "    # Generate the confusion matrix\n",
        "    matrix = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "\n",
        "    # Convert to DataFrame for easier labeling\n",
        "    df_cm = pd.DataFrame(matrix, index=labels, columns=labels)\n",
        "    df_cm.index.name = 'True Label'\n",
        "    df_cm.columns.name = 'Predicted Label'\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=figsize)\n",
        "    sn.set(font_scale=font_scale)\n",
        "    sn.heatmap(df_cm, cmap=cmap, annot=True, annot_kws={\"size\": annot_size}, fmt=\"d\")\n",
        "    plt.show()\n",
        "\n",
        "    return matrix\n"
      ],
      "metadata": {
        "id": "rE3HerUEwXl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def train_naive_bayes(x_train, y_train, x_test, y_test, var_smoothing=1e-9):\n",
        "\n",
        "    model = GaussianNB(var_smoothing=var_smoothing)\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    print(f'Naive Bayes Accuracy: {accuracy:.4f}')\n",
        "    print('Classification Report:\\n', report)\n",
        "\n",
        "    return model, accuracy, report\n"
      ],
      "metadata": {
        "id": "vV7YBkw_w4I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def train_random_forest(x_train, y_train, x_test, y_test,\n",
        "                        n_estimators=100, max_depth=10, min_samples_split=2,\n",
        "                        min_samples_leaf=1, max_features='auto', bootstrap=True,\n",
        "                        random_state=17):\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        bootstrap=bootstrap,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    print(f'Random Forest Accuracy: {accuracy:.4f}')\n",
        "    print('Classification Report:\\n', report)\n",
        "\n",
        "    return model, accuracy, report\n"
      ],
      "metadata": {
        "id": "XC973aUAxdQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def train_gbm(x_train, y_train, x_test, y_test,\n",
        "              n_estimators=100, learning_rate=0.1, max_depth=3,\n",
        "              min_samples_split=2, min_samples_leaf=1,\n",
        "              subsample=1.0, max_features=None,\n",
        "              random_state=17):\n",
        "\n",
        "    model = GradientBoostingClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        subsample=subsample,\n",
        "        max_features=max_features,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    print(f'GBM Accuracy: {accuracy:.4f}')\n",
        "    print('Classification Report:\\n', report)\n",
        "\n",
        "    return model, accuracy, report\n"
      ],
      "metadata": {
        "id": "L7QaZZxdxh5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Reshape, Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "def cnn_model():\n",
        "\n",
        "    x_train, y_train, x_test, y_test, n_features = datasets()\n",
        "    x_train, y_train = shuffle(x_train, y_train)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Reshape((n_features, 1), input_shape=(n_features,)))\n",
        "\n",
        "    # Convolutional Layers\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01), padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01), padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01), padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    lr_schedule = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=5000, decay_rate=0.9)\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        batch_size=64,\n",
        "        epochs=50,\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    _, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "    _, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "    print(f'Train: {train_acc:.3f}, Test: {test_acc:.3f}')\n",
        "\n",
        "    y_class_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    graph_acc(history)\n",
        "\n",
        "    matrix = conf_matrix(y_test, y_class_pred)\n",
        "\n",
        "    prediction_metrics(y_test, y_pred, y_class_pred, matrix)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "PE0hT2RWx1ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model1=cnn_model()"
      ],
      "metadata": {
        "id": "R9Z-vEe1yBxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Reshape, Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "def cnn_model_2d():\n",
        "\n",
        "    x_train, y_train, x_test, y_test, n_features = datasets()\n",
        "    x_train, y_train = shuffle(x_train, y_train)\n",
        "\n",
        "\n",
        "    img_size = int(n_features ** 0.5)\n",
        "    x_train = x_train.reshape(-1, img_size, img_size, 1)\n",
        "    x_test = x_test.reshape(-1, img_size, img_size, 1)\n",
        "\n",
        "    # Architecture\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "                     kernel_regularizer=l2(0.01), input_shape=(img_size, img_size, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "                     kernel_regularizer=l2(0.01)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "                     kernel_regularizer=l2(0.01)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    lr_schedule = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=5000, decay_rate=0.9)\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        batch_size=64,\n",
        "        epochs=50,\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    _, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "    _, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "    print(f'Train: {train_acc:.3f}, Test: {test_acc:.3f}')\n",
        "\n",
        "    y_class_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    graph_acc(history)\n",
        "\n",
        "    matrix = conf_matrix(y_test, y_class_pred)\n",
        "\n",
        "    prediction_metrics(y_test, y_pred, y_class_pred, matrix)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "HmM67b_qyID8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def vgg16_model(input_shape, num_classes):\n",
        "\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    lr_schedule = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9)\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def prepare_data():\n",
        "\n",
        "    x_train = np.random.rand(1000, 224, 224, 3)\n",
        "    y_train = np.random.randint(0, 2, 1000)\n",
        "    x_test = np.random.rand(200, 224, 224, 3)\n",
        "    y_test = np.random.randint(0, 2, 200)\n",
        "\n",
        "    y_train = to_categorical(y_train, num_classes=2)\n",
        "    y_test = to_categorical(y_test, num_classes=2)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "def train_vgg16_model():\n",
        "    x_train, y_train, x_test, y_test = prepare_data()\n",
        "\n",
        "    input_shape = (224, 224, 3)\n",
        "    num_classes = 2\n",
        "    model = vgg16_model(input_shape, num_classes)\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        batch_size=32,\n",
        "        epochs=50,\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    _, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "    _, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "    print(f'Train Accuracy: {train_acc:.3f}, Test Accuracy: {test_acc:.3f}')\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_class_pred = np.argmax(y_pred, axis=1)\n",
        "    y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "\n",
        "    graph_acc(history)\n",
        "    matrix = conf_matrix(y_test_labels, y_class_pred)\n",
        "    prediction_metrics(y_test_labels, y_pred, y_class_pred, matrix)\n",
        "\n",
        "    return model\n",
        "\n",
        "trained_model = train_vgg16_model()\n"
      ],
      "metadata": {
        "id": "VUZhQyogylCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def mlp_model():\n",
        "    x_train, y_train, x_test, y_test, n_features = datasets()\n",
        "    x_train, y_train = shuffle(x_train, y_train)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=n_features, activation='relu', kernel_regularizer='l2'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer='l2'))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    print(model.summary())\n",
        "    lr_schedule = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=5000, decay_rate=0.9)\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        batch_size=64,\n",
        "        epochs=50,\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    _, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "    _, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "    print(f'Train: {train_acc:.3f}, Test: {test_acc:.3f}')\n",
        "\n",
        "    y_class_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
        "    y_pred = model.predict(x_test)\n",
        "    graph_acc(history)\n",
        "    matrix = conf_matrix(y_test, y_class_pred)\n",
        "    prediction_metrics(y_test, y_pred, y_class_pred, matrix)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "zvw16aypzZtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import snntorch as snn\n",
        "import snntorch.utils as utils\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class SpikingCNN(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(SpikingCNN, self).__init__()\n",
        "\n",
        "        # Define network layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(128 * (input_shape[1] // 8), 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "        # Spiking neural network components\n",
        "        self.lif1 = snn.Leaky(beta=0.9, spike_grad=snntorch.gradiret.corrd, mem_update=True)\n",
        "        self.lif2 = snn.Leaky(beta=0.9, spike_grad=snntorch.gradiret.corrd, mem_update=True)\n",
        "        self.lif3 = snn.Leaky(beta=0.9, spike_grad=snntorch.gradiret.corrd, mem_update=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with spiking neurons\n",
        "        x = self.conv1(x)\n",
        "        x, _ = self.lif1(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x, _ = self.lif2(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x, _ = self.lif3(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_snn(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "    return losses\n",
        "\n",
        "def evaluate_snn(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).int()\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(outputs.cpu().numpy())\n",
        "    accuracy = correct / total\n",
        "    return accuracy, np.array(all_targets), np.array(all_predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "X29jtsVAzynT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    x_train, y_train, x_test, y_test, n_features = datasets()\n",
        "    x_train, x_test = x_train.reshape(-1, 1, n_features).astype(np.float32), x_test.reshape(-1, 1, n_features).astype(np.float32)\n",
        "    y_train, y_test = y_train.astype(np.float32), y_test.astype(np.float32)\n",
        "\n",
        "    train_dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
        "    test_dataset = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = SpikingCNN(input_shape=(1, n_features)).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_losses = train_snn(model, train_loader, optimizer, criterion, device)\n",
        "    accuracy, y_test, y_pred_prob = evaluate_snn(model, test_loader, device)\n",
        "\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    plot_metrics(history, train_losses)\n",
        "    matrix = conf_matrix(y_test, y_pred)\n",
        "    prediction_metrics(y_test, y_pred, matrix)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Ec3gOIyC0Ogn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(history, train_losses, val_losses):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.title('Training Loss vs Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin, y_pred_prob)\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0P7-JQ4B0PGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conf_matrix(y_test, y_pred):\n",
        "    matrix = confusion_matrix(y_test, y_pred)\n",
        "    df_cm = pd.DataFrame(matrix, columns=[0, 1], index=[0, 1])\n",
        "    df_cm.index.name = 'Truth'\n",
        "    df_cm.columns.name = 'Predicted'\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(df_cm, cmap=\"BuGn\", annot=True, annot_kws={\"size\": 16})\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "eu9-NnH_1FqI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}